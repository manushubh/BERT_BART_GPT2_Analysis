{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.bbc_gpt2_all import gpt2\n",
    "from ipynb.fs.defs.bbc_bert1_all import bert\n",
    "from ipynb.fs.defs.bbc_bart1_all import bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,model_name,folder,start,end):\n",
    "    final=\"\"\n",
    "    for i in range(start,end+1):\n",
    "        t1 = open(\"/Users/shreyabanerjee/Summarizer/multiple_docs/news/\"+folder+\"/\"+\"text\"+str(i),\"r\")\n",
    "        #print(i)\n",
    "        final=final+t1.read()\n",
    "    summary=model(final)\n",
    "    #t2=open(\"/Users/shreyabanerjee/Summarizer/multiple_docs/summary/\"+folder+\"/\"+model_name+\"/\"+\"summary.txt\",\"w\")\n",
    "    #t2.write(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the coronavirus number showing an increasing trend over the last two weeks, Maharashtra has reintroduced some restrictions on the assembly and movement of people. With 3,228 cases in that week (February 8-14), Pune had the highest number among these. But that does not explain the increase being witnessed in Vidarbha. For instance, the Tivsa tehsil in Amravati district is now showing 32.7 per cent positivity rate. This village has a total population of only 1,900. Awate suggested that marriage functions and other family events, that had to be postponed because of Covid-19 last year but are beginning to take place now, could also be contributing. “It is not uncommon to see gatherings of 400-500 at marriages or other events these days. But from now on, the rule of not more than 50 invitees would be strictly enforced. Also, people would have to wear masks at these functions,” Dr Rahul Pandit, a member of the state’s Covid-19 task force, said. Chief Minister Uddhav Thackeray and his deputy Ajit Pawar discussed the issue and the step is likely to be taken accordingly, according to the sources. The state govt has extended travel restrictions for those coming from Kerala, in the state. Among the districts, Amravati reported the highest rise in a day, from 82 cases on Tuesday to 230 cases on Wednesday.\n",
      "\n",
      "For the first time since mid-January, Maharashtra has reported more than 3,000 new cases of Covid-19 infections. Maharashtra has reintroduced some restrictions on the assembly and movement of people. The state government has warned that it could even bring back the lockdown if it was felt necessary. Nearly 60 per cent of the new infections in the second week of February were reported from Pune, Mumbai, Nagpur, Thane and Amravati. The spike seen in the last two weeks is a reversal of a declining trend that seemed to have become permanent in the state. The number of cases has dropped to less than the 140 million recommended in recent days, even though contact tracing efforts have also become weaker in the number of samples being tested for the disease. The last time the state reported 20,207 new cases was in the week ending this Sunday (February 14), compared to 17,672 in the previous week (February 1-7)\n",
      "\n",
      "With the coronavirus number showing an increasing trend over the last two weeks, Maharashtra has reintroduced some restrictions on the assembly and movement of people. With 3,228 cases in that week (February 8-14), Pune had the highest number among these. State surveillance officer Dr Pradeep Awate suggests the recently-held gram panchayat elections could also have played a role. That is one of the reasons why the restrictions on such gatherings have been brought back. “It is not uncommon to see gatherings of 400-500 at marriages or other events these days. But from now on, the rule of not more than 50 invitees would be strictly enforced. Also, people would have to wear masks at these functions,” Dr Rahul Pandit, a member of the state’s Covid-19 task force, said. In fact, like in the United States, there is a need to start using double-layered masks,” he said. The Maharashtra government may impose a stricter lockdown in Yavatmal, Amravati and Akola cities of Vidarbha region of the state \"at any moment\" in view of the COVID-19 situation there, news agency PTI reported. Maharashtra has been witnessing a surge in coronavirus cases. It has made it mandatory for travellers to carry a Covid-negative report while entering the state.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_summary=[generate(gpt2,\"gpt2\",\"Covid\",1,2)]\n",
    "bart_summary=[generate(bart,\"bart\",\"Covid\",1,2)]\n",
    "bert_summary=[generate(bert,\"bert\",\"Covid\",1,2)]\n",
    "print(gpt2_summary[0]+'\\n')\n",
    "print(bart_summary[0]+'\\n')\n",
    "print(bert_summary[0]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = open(\"/Users/shreyabanerjee/Summarizer/multiple_docs/human_written_summary/Covid/summary.txt\",\"r\")\n",
    "my_summary=[m1.read()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Human written summary\\n\"+my_summary[0]+\"\\n\")\n",
    "print(\"Gpt2 summary\\n\"+gpt2_summary[0]+\"\\n\")\n",
    "print(\"Bert summary\\n\"+bert_summary[0]+\"\\n\")\n",
    "print(\"Bart summary\\n\"+bart_summary[0]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metric\n",
    "def get_score(all_hypothesis, all_references):\n",
    "    apply_avg = 'Avg'\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                               max_n=4,\n",
    "                               limit_length=True,\n",
    "                               length_limit=100,\n",
    "                               length_limit_type='words',\n",
    "                               apply_avg=apply_avg,\n",
    "                               alpha=0.5, # Default F1_score\n",
    "                               weight_factor=1.2,\n",
    "                               stemming=True)\n",
    "    scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "    for key,value in scores.items():\n",
    "        print(key.upper()+'\\n')\n",
    "        print(\"F: \"+str(value['f'])+\"  R: \"+str(value['r'])+\"  P: \"+str(value['p'])+\"\\n\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1\n",
      "\n",
      "F: 0.375  R: 0.375  P: 0.375\n",
      "\n",
      "ROUGE-2\n",
      "\n",
      "F: 0.08737864077669903  R: 0.08737864077669903  P: 0.08737864077669903\n",
      "\n",
      "ROUGE-3\n",
      "\n",
      "F: 0.029411764705882353  R: 0.029411764705882353  P: 0.029411764705882353\n",
      "\n",
      "ROUGE-4\n",
      "\n",
      "F: 0.019801980198019802  R: 0.019801980198019802  P: 0.019801980198019802\n",
      "\n",
      "ROUGE-L\n",
      "\n",
      "F: 0.26362603402756773  R: 0.26362603402756773  P: 0.26362603402756773\n",
      "\n",
      "ROUGE-W\n",
      "\n",
      "F: 0.07873432746224886  R: 0.054917058652875425  P: 0.13903173304376693\n",
      "\n",
      "{'rouge-1': {'f': 0.375, 'p': 0.375, 'r': 0.375}, 'rouge-2': {'f': 0.08737864077669903, 'p': 0.08737864077669903, 'r': 0.08737864077669903}, 'rouge-3': {'f': 0.029411764705882353, 'p': 0.029411764705882353, 'r': 0.029411764705882353}, 'rouge-4': {'f': 0.019801980198019802, 'p': 0.019801980198019802, 'r': 0.019801980198019802}, 'rouge-l': {'f': 0.26362603402756773, 'p': 0.26362603402756773, 'r': 0.26362603402756773}, 'rouge-w': {'f': 0.07873432746224886, 'p': 0.13903173304376693, 'r': 0.054917058652875425}}\n"
     ]
    }
   ],
   "source": [
    "print(get_score(bert_summary,my_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1\n",
      "\n",
      "F: 0.5024154589371981  R: 0.5  P: 0.5048543689320388\n",
      "\n",
      "ROUGE-2\n",
      "\n",
      "F: 0.38048780487804873  R: 0.3786407766990291  P: 0.38235294117647056\n",
      "\n",
      "ROUGE-3\n",
      "\n",
      "F: 0.35467980295566504  R: 0.35294117647058826  P: 0.3564356435643564\n",
      "\n",
      "ROUGE-4\n",
      "\n",
      "F: 0.3383084577114428  R: 0.33663366336633666  P: 0.34\n",
      "\n",
      "ROUGE-L\n",
      "\n",
      "F: 0.4902613341900243  R: 0.48829556636775173  P: 0.4922429934635\n",
      "\n",
      "ROUGE-W\n",
      "\n",
      "F: 0.2066448891929225  R: 0.1437420333521476  P: 0.3674401134306584\n",
      "\n",
      "{'rouge-1': {'f': 0.5024154589371981, 'p': 0.5048543689320388, 'r': 0.5}, 'rouge-2': {'f': 0.38048780487804873, 'p': 0.38235294117647056, 'r': 0.3786407766990291}, 'rouge-3': {'f': 0.35467980295566504, 'p': 0.3564356435643564, 'r': 0.35294117647058826}, 'rouge-4': {'f': 0.3383084577114428, 'p': 0.34, 'r': 0.33663366336633666}, 'rouge-l': {'f': 0.4902613341900243, 'p': 0.4922429934635, 'r': 0.48829556636775173}, 'rouge-w': {'f': 0.2066448891929225, 'p': 0.3674401134306584, 'r': 0.1437420333521476}}\n"
     ]
    }
   ],
   "source": [
    "print(get_score(bart_summary,my_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1\n",
      "\n",
      "F: 0.3540669856459331  R: 0.3557692307692308  P: 0.3523809523809524\n",
      "\n",
      "ROUGE-2\n",
      "\n",
      "F: 0.08695652173913045  R: 0.08737864077669903  P: 0.08653846153846154\n",
      "\n",
      "ROUGE-3\n",
      "\n",
      "F: 0.03902439024390244  R: 0.0392156862745098  P: 0.038834951456310676\n",
      "\n",
      "ROUGE-4\n",
      "\n",
      "F: 0.029556650246305417  R: 0.0297029702970297  P: 0.029411764705882353\n",
      "\n",
      "ROUGE-L\n",
      "\n",
      "F: 0.2625748910900313  R: 0.26362603402756773  P: 0.2615320972023661\n",
      "\n",
      "ROUGE-W\n",
      "\n",
      "F: 0.07917610353893967  R: 0.05537555387316975  P: 0.1388573239195547\n",
      "\n",
      "{'rouge-1': {'f': 0.3540669856459331, 'p': 0.3523809523809524, 'r': 0.3557692307692308}, 'rouge-2': {'f': 0.08695652173913045, 'p': 0.08653846153846154, 'r': 0.08737864077669903}, 'rouge-3': {'f': 0.03902439024390244, 'p': 0.038834951456310676, 'r': 0.0392156862745098}, 'rouge-4': {'f': 0.029556650246305417, 'p': 0.029411764705882353, 'r': 0.0297029702970297}, 'rouge-l': {'f': 0.2625748910900313, 'p': 0.2615320972023661, 'r': 0.26362603402756773}, 'rouge-w': {'f': 0.07917610353893967, 'p': 0.1388573239195547, 'r': 0.05537555387316975}}\n"
     ]
    }
   ],
   "source": [
    "print(get_score(gpt2_summary,my_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to preprocess the files in the document cluster before\n",
    "#\t\t\t\t  passing them into the MMR summarizer system. Here the sentences\n",
    "#\t\t\t\t  of the document cluster are modelled as sentences after extracting\n",
    "#\t\t\t\t  from the files in the folder path. \n",
    "# Parameters\t: file_name, name of the file in the document cluster\n",
    "# Return \t\t: list of sentence object\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to find the term frequencies of the words in the\n",
    "#\t\t\t\t  sentences present in the provided document cluster\n",
    "# Parameters\t: sentences, sentences of the document cluster\n",
    "# Return \t\t: dictonary of word, term frequency score\n",
    "#---------------------------------------------------------------------------------\n",
    "def TFs(sentences):\n",
    "\t# initialize tfs dictonary\n",
    "\ttfs = {}\n",
    "\n",
    "\t# for every sentence in document cluster\n",
    "\tfor sent in sentences:\n",
    "\t\t# retrieve word frequencies from sentence object\n",
    "\t    wordFreqs = sent.getWordFreq()\n",
    "\t    \n",
    "\t    # for every word\n",
    "\t    for word in wordFreqs.keys():\n",
    "\t    \t# if word already present in the dictonary\n",
    "\t        if tfs.get(word, 0) != 0:\t\t\t\t\n",
    "\t\t\t\ttfs[word] = tfs[word] + wordFreqs[word]\n",
    "\t        # else if word is being added for the first time\n",
    "\t        else:\t\t\t\t\n",
    "\t\t\t\ttfs[word] = wordFreqs[word]\t\n",
    "\treturn tfs\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to find the inverse document frequencies of the words in\n",
    "#\t\t\t\t  the sentences present in the provided document cluster \n",
    "# Parameters\t: sentences, sentences of the document cluster\n",
    "# Return \t\t: dictonary of word, inverse document frequency score\n",
    "#---------------------------------------------------------------------------------\n",
    "def IDFs(sentences):\n",
    "    N = len(sentences)\n",
    "    idf = 0\n",
    "    idfs = {}\n",
    "    words = {}\n",
    "    w2 = []\n",
    "    \n",
    "    # every sentence in our cluster\n",
    "    for sent in sentences:\n",
    "        \n",
    "        # every word in a sentence\n",
    "        for word in sent.getPreProWords():\n",
    "\n",
    "            # not to calculate a word's IDF value more than once\n",
    "            if sent.getWordFreq().get(word, 0) != 0:\n",
    "                words[word] = words.get(word, 0)+ 1\n",
    "\n",
    "    # for each word in words\n",
    "    for word in words:\n",
    "        n = words[word]\n",
    "        \n",
    "        # avoid zero division errors\n",
    "        try:\n",
    "            w2.append(n)\n",
    "            idf = math.log10(float(N)/n)\n",
    "        except ZeroDivisionError:\n",
    "            idf = 0\n",
    "                \n",
    "        # reset variables\n",
    "        idfs[word] = idf\n",
    "            \n",
    "    return idfs\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to find TF-IDF score of the words in the document cluster\n",
    "# Parameters\t: sentences, sentences of the document cluster\n",
    "# Return \t\t: dictonary of word, TF-IDF score\n",
    "#---------------------------------------------------------------------------------\n",
    "def TF_IDF(sentences):\n",
    "    # Method variables\n",
    "    tfs = TFs(sentences)\n",
    "    idfs = IDFs(sentences)\n",
    "    retval = {}\n",
    "\n",
    "    # for every word\n",
    "    for word in tfs:\n",
    "        #calculate every word's tf-idf score\n",
    "        tf_idfs=  tfs[word] * idfs[word]\n",
    "        \n",
    "        # add word and its tf-idf score to dictionary\n",
    "        if retval.get(tf_idfs, None) == None:\n",
    "            retval[tf_idfs] = [word]\n",
    "        else:\n",
    "            retval[tf_idfs].append(word)\n",
    "\n",
    "    return retval\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to find the sentence similarity for a pair of sentences\n",
    "#\t\t\t\t  by calculating cosine similarity\n",
    "# Parameters\t: sentence1, first sentence\n",
    "#\t\t\t\t  sentence2, second sentence to which first sentence has to be compared\n",
    "#\t\t\t\t  IDF_w, dictinoary of IDF scores of words in the document cluster\n",
    "# Return \t\t: cosine similarity score\n",
    "#---------------------------------------------------------------------------------\n",
    "def sentenceSim(sentence1, sentence2, IDF_w):\n",
    "\tnumerator = 0\n",
    "\tdenominator = 0\t\n",
    "\t\n",
    "\tfor word in sentence2.getPreProWords():\t\t\n",
    "\t\tnumerator+= sentence1.getWordFreq().get(word,0) * sentence2.getWordFreq().get(word,0) *  IDF_w.get(word,0) ** 2\n",
    "\n",
    "\tfor word in sentence1.getPreProWords():\n",
    "\t\tdenominator+= ( sentence1.getWordFreq().get(word,0) * IDF_w.get(word,0) ) ** 2\n",
    "\n",
    "\t# check for divide by zero cases and return back minimal similarity\n",
    "\ttry:\n",
    "\t\treturn numerator / math.sqrt(denominator)\n",
    "\texcept ZeroDivisionError:\n",
    "\t\treturn float(\"-inf\")\t\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to build a query of n words on the basis of TF-IDF value\n",
    "# Parameters\t: sentences, sentences of the document cluster\n",
    "#\t\t\t\t  IDF_w, IDF values of the words\n",
    "#\t\t\t\t  n, desired length of query (number of words in query)\n",
    "# Return \t\t: query sentence consisting of best n words\n",
    "#---------------------------------------------------------------------------------\n",
    "def buildQuery(sentences, TF_IDF_w, n):\n",
    "\t#sort in descending order of TF-IDF values\n",
    "\tscores = TF_IDF_w.keys()\n",
    "\tscores.sort(reverse=True)\t\n",
    "\t\n",
    "\ti = 0\n",
    "\tj = 0\n",
    "\tqueryWords = []\n",
    "\n",
    "\t# select top n words\n",
    "\twhile(i<n):\n",
    "\t\twords = TF_IDF_w[scores[j]]\n",
    "\t\tfor word in words:\n",
    "\t\t\tqueryWords.append(word)\n",
    "\t\t\ti=i+1\n",
    "\t\t\tif (i>n): \n",
    "\t\t\t\tbreak\n",
    "\t\tj=j+1\n",
    "\n",
    "\t# return the top selected words as a sentence\n",
    "\treturn sentence.sentence(\"query\", queryWords, queryWords)\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to find the best sentence in reference to the query\n",
    "# Parameters\t: sentences, sentences of the document cluster\n",
    "#\t\t\t\t  query, reference query\n",
    "#\t\t\t\t  IDF, IDF value of words of the document cluster\n",
    "# Return \t\t: best sentence among the sentences in the document cluster\n",
    "#---------------------------------------------------------------------------------\n",
    "def bestSentence(sentences, query, IDF):\n",
    "\tbest_sentence = None\n",
    "\tmaxVal = float(\"-inf\")\n",
    "\n",
    "\tfor sent in sentences:\n",
    "\t\tsimilarity = sentenceSim(sent, query, IDF)\t\t\n",
    "\n",
    "\t\tif similarity > maxVal:\n",
    "\t\t\tbest_sentence = sent\n",
    "\t\t\tmaxVal = similarity\n",
    "\tsentences.remove(best_sentence)\n",
    "\n",
    "\treturn best_sentence\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to create the summary set of a desired number of words \n",
    "# Parameters\t: sentences, sentences of the document cluster\n",
    "#\t\t\t\t  best_sentnece, best sentence in the document cluster\n",
    "#\t\t\t\t  query, reference query for the document cluster\n",
    "#\t\t\t\t  summary_length, desired number of words for the summary\n",
    "#\t\t\t\t  labmta, lambda value of the MMR score calculation formula\n",
    "#\t\t\t\t  IDF, IDF value of words in the document cluster \n",
    "# Return \t\t: name \n",
    "#---------------------------------------------------------------------------------\n",
    "def makeSummary(sentences, best_sentence, query, summary_length, lambta, IDF):\t\n",
    "\tsummary = [best_sentence]\n",
    "\tsum_len = len(best_sentence.getPreProWords())\n",
    "\n",
    "\tMMRval={}\n",
    "\n",
    "\t# keeping adding sentences until number of words exceeds summary length\n",
    "\twhile (sum_len < summary_length):\t\n",
    "\t\tMMRval={}\t\t\n",
    "\n",
    "\t\tfor sent in sentences:\n",
    "\t\t\tMMRval[sent] = MMRScore(sent, query, summary, lambta, IDF)\n",
    "\t\t\n",
    "\t\tmaxxer = max(MMRval, key=MMRval.get)\n",
    "\t \tsummary.append(maxxer)\n",
    "\t\tsentences.remove(maxxer)\n",
    "\t\tsum_len += len(maxxer.getPreProWords())\t\n",
    "\n",
    "\treturn summary\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Description\t: Function to calculate the MMR score given a sentence, the query\n",
    "#\t\t\t\t  and the current best set of sentences\n",
    "# Parameters\t: Si, particular sentence for which the MMR score has to be calculated\n",
    "#\t\t\t\t  query, query sentence for the particualr document cluster\n",
    "#\t\t\t\t  Sj, the best sentences that are already selected\n",
    "#\t\t\t\t  lambta, lambda value in the MMR formula\n",
    "#\t\t\t\t  IDF, IDF value for words in the cluster\n",
    "# Return \t\t: name \n",
    "#---------------------------------------------------------------------------------\n",
    "def MMRScore(Si, query, Sj, lambta, IDF):\t\n",
    "\tSim1 = sentenceSim(Si, query, IDF)\n",
    "\tl_expr = lambta * Sim1\n",
    "\tvalue = [float(\"-inf\")]\n",
    "\n",
    "\tfor sent in Sj:\n",
    "\t\tSim2 = sentenceSim(Si, sent, IDF)\n",
    "\t\tvalue.append(Sim2)\n",
    "\n",
    "\tr_expr = (1-lambta) * max(value)\n",
    "\tMMR_SCORE = l_expr - r_expr\t\n",
    "\n",
    "\treturn MMRScore\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "#\tMAIN FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "if __name__=='__main__':\t\n",
    "\n",
    "\t# set the main Document folder path where the subfolders are present\n",
    "\tmain_folder_path = os.getcwd() + \"/Documents\"\n",
    "\n",
    "\t# read in all the subfolder names present in the main folder\n",
    "\tfor folder in os.listdir(main_folder_path):\n",
    "\t\t\n",
    "\t\tprint \"Running MMR Summarizer for files in folder: \", folder \n",
    "\t\t# for each folder run the MMR summarizer and generate the final summary\n",
    "\t\tcurr_folder = main_folder_path + \"/\" + folder\t\t\n",
    "\n",
    "\t\t# find all files in the sub folder selected\n",
    "\t\tfiles = os.listdir(curr_folder)\n",
    "\n",
    "\t\tsentences = []\t\n",
    "\n",
    "\t\tfor file in files:\t\t\t\n",
    "\t\t\tsentences = sentences + processFile(curr_folder + \"/\" + file)\n",
    "\n",
    "\t\t# calculate TF, IDF and TF-IDF scores\n",
    "\t\t# TF_w \t\t= TFs(sentences)\n",
    "\t\tIDF_w \t\t= IDFs(sentences)\n",
    "\t\tTF_IDF_w \t= TF_IDF(sentences)\t\n",
    "\n",
    "\t\t# build query; set the number of words to include in our query\n",
    "\t\tquery = buildQuery(sentences, TF_IDF_w, 10)\t\t\n",
    "\n",
    "\t\t# pick a sentence that best matches the query\t\n",
    "\t\tbest1sentence = bestSentence(sentences, query, IDF_w)\t\t\n",
    "\n",
    "\t\t# build summary by adding more relevant sentences\n",
    "\t\tsummary = makeSummary(sentences, best1sentence, query, 100, 0.5, IDF_w)\n",
    "\t\t\n",
    "\t\tfinal_summary = \"\"\n",
    "\t\tfor sent in summary:\n",
    "\t\t\tfinal_summary = final_summary + sent.getOriginalWords() + \"\\n\"\n",
    "\t\tfinal_summary = final_summary[:-1]\n",
    "\t\tresults_folder = os.getcwd() + \"/MMR_results\"\t\t\n",
    "\t\twith open(os.path.join(results_folder,(str(folder) + \".MMR\")),\"w\") as fileOut: fileOut.write(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import sentence\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''With the coronavirus number showing an increasing trend over the last two weeks, Maharashtra has reintroduced some restrictions on the assembly and movement of people. The state government has warned that it could even bring back the lockdown if it was felt necessary.\n",
    "\n",
    "What is the Covid-19 situation in Maharashtra?\n",
    "Every day of the last week, for the first time since mid-January, Maharashtra has reported more than 3,000 new cases of coronavirus infections. The new cases in the second week of February were at least 14 per cent more than the first. In the week ending this Sunday (February 14), 20,207 new cases were detected across the state, compared to 17,672 in the previous week (February 1-7). In the week prior to that (January 25-31), the state had reported 17,293 cases.\n",
    "\n",
    "Mumbai, Pune, their surrounding areas along with the Vidarbha region have contributed the maximum to this surge. Nearly 60 per cent of the new infections in the second week of February were reported from Pune, Mumbai, Nagpur, Thane and Amravati. With 3,228 cases in that week (February 8-14), Pune had the highest number among these. Nagpur (2,628 cases) and Amravati (2,420) discovered more cases than Mumbai (2,195). Thane reported 1,960 cases during that week.\n",
    "In the last one week, Mumbai and Pune have both reported more than 600 cases in a day, something that they had not done in at least a month. Nagpur has reported more than 500 cases twice in the last week while Thane has gone past 400.\n",
    "\n",
    "The rise is not very alarming right now. Maharashtra had been reporting between 2,000 and 2,500 cases through most of January, but the numbers were steadily, though very slowly, going down. The spike seen in the last two weeks is a reversal of a declining trend that seemed to have become permanent.\n",
    "\n",
    "“It is not a red signal yet, but definitely a yellow warning. We have to take steps to ensure that this does not become red,” Dr Shahank Joshi, a member of the state’s Covid-19 task force, said.\n",
    "One important reason could be the reopening of the local trains for the general public in the Mumbai region. But that does not explain the increase being witnessed in Vidarbha. State surveillance officer Dr Pradeep Awate suggests the recently-held gram panchayat elections could also have played a role.“Some areas in Vidarbha and Marathwada had reported more than 80 per cent voter turnout during the gram panchayat elections. It could have led to the spread of the disease. For instance, the Tivsa tehsil in Amravati district is now showing 32.7 per cent positivity rate. That means, every third sample is testing positive,” Awate said.\n",
    "\n",
    "“Similarly, the Sasurve village in Rahmatpur, Koregaon tehsil, of Satara district recently reported 62 new infections. This village has a total population of only 1,900. Election campaigning and voting had seen good crowds in these areas,” Awate, who has been visiting some of these areas to understand the reasons for the surge, said.\n",
    "\n",
    "Awate suggested that marriage functions and other family events, that had to be postponed because of Covid-19 last year but are beginning to take place now, could also be contributing. That is one of the reasons why the restrictions on such gatherings have been brought back.“It is not uncommon to see gatherings of 400-500 at marriages or other events these days. But from now on, the rule of not more than 50 invitees would be strictly enforced. Also, people would have to wear masks at these functions,” Dr Rahul Pandit, a member of the state’s Covid-19 task force, said.\n",
    "\n",
    "Dr Shashank Joshi said there was also a mistaken belief that the epidemic was over. “We cannot let our guard down and use Covid fatigue as an excuse to not wear masks. In fact, like in the United States, there is a need to start using double-layered masks,” he said.\n",
    "\n",
    "State health minister Rajesh Tope agreed, and said people not following physical distancing norms, or avoiding face masks, would be heavily penalised. He said the district administration had authorised to shut schools if the situation so warranted.\n",
    "\n",
    "Interestingly, there has been a drop in the number of samples being tested. Contact tracing efforts have also become weaker. Districts like Sindhudurg, Wardha, Palghar, Osmanabad, Nandurbar and Chandrapure have reported very low testing in recent days, even less than the recommended 140 per million.\n",
    "\n",
    "“Our teams are now at various places across the 14 districts that have shown an increase in numbers, and identifying the most vulnerable spots to take necessary actions,” Dr Awate said.“It is not uncommon to see gatherings of 400-500 at marriages or other events these days. But from now on, the rule of not more than 50 invitees would be strictly enforced. Also, people would have to wear masks at these functions,” Dr Rahul Pandit, a member of the state’s Covid-19 task force, said.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFile(filename):\n",
    "    # segment data into a list of sentences\n",
    "    f = open(filename,'r')\n",
    "    text=f.read();\n",
    "    sentence_token = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    lines = sentence_token.tokenize(text.strip())\n",
    "    #print(lines)\n",
    "    sentences = []\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    # modelling each sentence in file as sentence object\n",
    "    #print(lines)\n",
    "    for line in lines:\n",
    "\n",
    "        # original words of the sentence before stemming\n",
    "        originalWords = line[:]\n",
    "        line = line.strip().lower()\n",
    "\n",
    "        # word tokenization\n",
    "        sent = nltk.word_tokenize(line)\n",
    "        #print(sent)\n",
    "        # stemming words\n",
    "        stemmedSent = [porter.stem(word) for word in sent]\n",
    "        #print(stemmedSent)\n",
    "        for x in stemmedSent:\n",
    "                if(x=='.'or x=='`'or x==','or x=='?'or x==\"'\" or x=='!' or x=='''\"''' or x==\"''\" or x==\"'s\"):\n",
    "                    #print(x)\n",
    "                    stemmedSent.remove(x)\n",
    "        #stemmedSent = filter(lambda x: x!='.'and x!='`'and x!=','and x!='?'and x!=\"'\" and x!='!' and x!='''\"''' and x!=\"''\" and x!=\"'s\", stemmedSent)\n",
    "        #print(stemmedSent)\n",
    "\n",
    "        #list of sentence objects\n",
    "        if stemmedSent != []:\n",
    "            sent=sentence.sentence(filename,stemmedSent, originalWords)\n",
    "            sentences.append(sent)\n",
    "            #print(sent.originalWords)\n",
    "            #print(sent.preproWords)\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sentence.sentence at 0x7fbf5b8ecc10>,\n",
       " <sentence.sentence at 0x7fbf5b8ec4c0>,\n",
       " <sentence.sentence at 0x7fbf5b8ec340>,\n",
       " <sentence.sentence at 0x7fbf5b8ec370>,\n",
       " <sentence.sentence at 0x7fbf5b8ec7c0>,\n",
       " <sentence.sentence at 0x7fbf5b9eddc0>,\n",
       " <sentence.sentence at 0x7fbf5b8ec430>,\n",
       " <sentence.sentence at 0x7fbf5b8ec880>,\n",
       " <sentence.sentence at 0x7fbf5b8ec1f0>,\n",
       " <sentence.sentence at 0x7fbf5b8ec220>,\n",
       " <sentence.sentence at 0x7fbf5b8ec1c0>,\n",
       " <sentence.sentence at 0x7fbf5b8ec310>,\n",
       " <sentence.sentence at 0x7fbf5b8ec2b0>,\n",
       " <sentence.sentence at 0x7fbf5b8ecc70>,\n",
       " <sentence.sentence at 0x7fbf5b8ec2e0>,\n",
       " <sentence.sentence at 0x7fbf5b8ec130>,\n",
       " <sentence.sentence at 0x7fbf5b8eca90>,\n",
       " <sentence.sentence at 0x7fbf5b8ec160>,\n",
       " <sentence.sentence at 0x7fbf5b8eca60>,\n",
       " <sentence.sentence at 0x7fbf5b8ecd30>,\n",
       " <sentence.sentence at 0x7fbf5b8ec490>,\n",
       " <sentence.sentence at 0x7fbf5ba38430>,\n",
       " <sentence.sentence at 0x7fbf5b8ec070>,\n",
       " <sentence.sentence at 0x7fbf5b8ec400>,\n",
       " <sentence.sentence at 0x7fbf5b8ec250>,\n",
       " <sentence.sentence at 0x7fbf5b8ec610>,\n",
       " <sentence.sentence at 0x7fbf5b8d7040>,\n",
       " <sentence.sentence at 0x7fbf5b8d7340>,\n",
       " <sentence.sentence at 0x7fbf5b8d70a0>,\n",
       " <sentence.sentence at 0x7fbf5b8d73a0>,\n",
       " <sentence.sentence at 0x7fbf5b8d72e0>,\n",
       " <sentence.sentence at 0x7fbf5b8d70d0>,\n",
       " <sentence.sentence at 0x7fbf5b8df5b0>,\n",
       " <sentence.sentence at 0x7fbf5b8dff10>,\n",
       " <sentence.sentence at 0x7fbf5b8df8e0>,\n",
       " <sentence.sentence at 0x7fbf5b8df6a0>,\n",
       " <sentence.sentence at 0x7fbf5b8dff40>,\n",
       " <sentence.sentence at 0x7fbf5b8df910>,\n",
       " <sentence.sentence at 0x7fbf5b8dfca0>,\n",
       " <sentence.sentence at 0x7fbf5b8df7c0>,\n",
       " <sentence.sentence at 0x7fbf5b8dfc70>,\n",
       " <sentence.sentence at 0x7fbf5b8df2e0>,\n",
       " <sentence.sentence at 0x7fbf5b8dfee0>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processFile('001.txt',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFs(sentences):\n",
    "    # initialize tfs dictonary\n",
    "    tfs = {}\n",
    "\n",
    "    # for every sentence in document cluster\n",
    "    for sent in sentences:\n",
    "        # retrieve word frequencies from sentence object\n",
    "        wordFreqs = sent.getWordFreq()\n",
    "        # for every word\n",
    "        for word in wordFreqs.keys():\n",
    "            # if word already present in the dictonary\n",
    "            if tfs.get(word, 0) != 0:\n",
    "                tfs[word] = tfs[word] + wordFreqs[word]\n",
    "                # else if word is being added for the first time\n",
    "            else:\n",
    "                tfs[word] = wordFreqs[word]\t\n",
    "    return tfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDFs(sentences):\n",
    "    N = len(sentences)\n",
    "    idf = 0\n",
    "    idfs = {}\n",
    "    words = {}\n",
    "    w2 = []\n",
    "    \n",
    "    # every sentence in our cluster\n",
    "    for sent in sentences:\n",
    "        #print(sent)\n",
    "        # every word in a sentence\n",
    "        for word in sent.getPreProWords():\n",
    "            #print(word)\n",
    "\n",
    "            # not to calculate a word's IDF value more than once\n",
    "            if sent.getWordFreq().get(word, 0) != 0:\n",
    "                words[word] = words.get(word, 0)+ 1\n",
    "\n",
    "    #print(words)\n",
    "    # for each word in words\n",
    "    for word in words:\n",
    "        n = words[word]\n",
    "        \n",
    "        # avoid zero division errors\n",
    "        try:\n",
    "            w2.append(n)\n",
    "            idf = math.log10(float(N)/n)\n",
    "        except ZeroDivisionError:\n",
    "            idf = 0\n",
    "                \n",
    "        # reset variables\n",
    "        idfs[word] = idf\n",
    "            \n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(sentences):\n",
    "    # Method variables\n",
    "    tfs = TFs(sentences)\n",
    "    idfs = IDFs(sentences)\n",
    "    #print(idfs)\n",
    "    retval = {}\n",
    "\n",
    "    # for every word\n",
    "    for word in tfs:\n",
    "        #calculate every word's tf-idf score\n",
    "        tf_idfs=  tfs[word] * idfs[word]\n",
    "        \n",
    "        # add word and its tf-idf score to dictionary\n",
    "        if retval.get(tf_idfs, None) == None:\n",
    "            retval[tf_idfs] = [word]\n",
    "        else:\n",
    "            retval[tf_idfs].append(word)\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSim(sentence1, sentence2, IDF_w):\n",
    "    numerator = 0\n",
    "    denominator = 0\t\n",
    "    \n",
    "    for word in sentence2.getPreProWords():\t\t\n",
    "        numerator+= sentence1.getWordFreq().get(word,0) * sentence2.getWordFreq().get(word,0) *  IDF_w.get(word,0) ** 2\n",
    "\n",
    "    for word in sentence1.getPreProWords():\n",
    "        denominator+= ( sentence1.getWordFreq().get(word,0) * IDF_w.get(word,0) ) ** 2\n",
    "\n",
    "    # check for divide by zero cases and return back minimal similarity\n",
    "    try:\n",
    "        return numerator / math.sqrt(denominator)\n",
    "    except ZeroDivisionError:\n",
    "        return float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildQuery(sentences, TF_IDF_w, n):\n",
    "    #sort in descending order of TF-IDF values\n",
    "    scores = TF_IDF_w.keys()\n",
    "    #print(scores)\n",
    "    scores=sorted(scores,reverse=True)\n",
    "    #print(scores)\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    queryWords = []\n",
    "\n",
    "    # select top n words\n",
    "    while(i<n):\n",
    "        words = TF_IDF_w[scores[j]]\n",
    "        for word in words:\n",
    "            queryWords.append(word)\n",
    "            i=i+1\n",
    "            if (i>n): \n",
    "                break\n",
    "        j=j+1\n",
    "\n",
    "    # return the top selected words as a sentence\n",
    "    return sentence.sentence(\"query\", queryWords, queryWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSentence(sentences, query, IDF):\n",
    "    best_sentence = None\n",
    "    maxVal = float(\"-inf\")\n",
    "    for sent in sentences:\n",
    "        similarity = sentenceSim(sent, query, IDF)\n",
    "\n",
    "        if similarity > maxVal:\n",
    "            best_sentence = sent\n",
    "            maxVal = similarity\n",
    "    sentences.remove(best_sentence)\n",
    "    return best_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSummary(sentences, best_sentence, query, summary_length, lambta, IDF):\n",
    "    summary = [best_sentence]\n",
    "    sum_len = len(best_sentence.getPreProWords())\n",
    "\n",
    "    MMRval={}\n",
    "\n",
    "    # keeping adding sentences until number of words exceeds summary length\n",
    "    while (sum_len < summary_length):\n",
    "        MMRval={}\n",
    "        #print(sentences.OriginalWords)\n",
    "        for sent in sentences:\n",
    "            #print(\"Summary=\",summary)\n",
    "            \n",
    "            mmr= MMRScore(sent, query, summary, lambta, IDF)\n",
    "            MMRval[sent]=mmr\n",
    "            #print(MMRval[sent])\n",
    "            print(\"Inside function mmr= \",mmr)\n",
    "        print(\"Out of loop\")\n",
    "        '''print(\"MMRVAL=\",MMRval)\n",
    "        print(\"\\n\\n\\n\",MMRval.get)\n",
    "        maxxer = max(MMRval, key=MMRval.get)\n",
    "        summary.append(maxxer)\n",
    "        sentences.remove(maxxer)\n",
    "        sum_len += len(maxxer.getPreProWords())'''\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMRScore(Si, query, Sj, lambta, IDF):\n",
    "    #print(\"Sj= \",Sj)\n",
    "    print(\"In an infite loop\")\n",
    "    Sim1 = sentenceSim(Si, query, IDF)\n",
    "    l_expr = lambta * Sim1\n",
    "    value = [float(\"-inf\")]\n",
    "    \n",
    "    for sent in Sj:\n",
    "        Sim2 = sentenceSim(Si, sent, IDF)\n",
    "        value.append(Sim2)\n",
    "\n",
    "    r_expr = (1-lambta) * max(value)\n",
    "    MMR_SCORE = l_expr - r_expr\t\n",
    "    #print(\"MMRSCORE   \",MMR_SCORE)\n",
    "    print(\"\\nEnd of function\\n\")\n",
    "    return MMRScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MMR Summarizer for files in folder:  Topic1\n",
      "\"An FIR was registered against Hitesha, the woman, who claimed to be attacked by a Zomato delivery man,\" a police officer said, as quoted by PTI.In the complaint, Kamraj alleged Chandranee had hit him with slippers, accused him of defaming her, and hurling abuses at him, the officer said.\n",
      "Running MMR Summarizer for files in folder:  Topic2\n",
      "Police sought death penalty for Ariz, allegedly associated with the terror outfit Indian Mujahideen, saying it was not just any killing but a murder of a law enforcement officer who was a defender of justice.\n",
      "Running MMR Summarizer for files in folder:  Topic3\n",
      "\"Our results show that what we have experienced over the past five summers is extraordinary for central Europe, in terms of how dry it has been consecutively,\" he said.The new study, published in the science journal Nature Geoscience, further shows how climate change impacts our day-to-day lives.\n",
      "Running MMR Summarizer for files in folder:  Topic4\n",
      "Prime Minister Narendra Modi tweeted to say that he was pained by Mr Sharma's untimely and unfortunate demise.\n",
      "Running MMR Summarizer for files in folder:  Topic5\n",
      "The proposal of Banaras Hindu University (BHU) to appoint Nita Ambani, the wife of billionaire industrialist Mukesh Ambani, as a visiting professor is being opposed by students who see it as a “wrong example” being set by the university.\n"
     ]
    }
   ],
   "source": [
    "# set the main Document folder path where the subfolders are present\n",
    "main_folder_path = \"/Users/shreyabanerjee/Summarizer/multiple_docs/news\"\n",
    "\n",
    "# read in all the subfolder names present in the main folder\n",
    "lis=[]\n",
    "for i in range(1,6):\n",
    "    lis.append(\"Topic\"+str(i))\n",
    "j=1\n",
    "for folder in lis:\n",
    "        print(\"Running MMR Summarizer for files in folder: \", folder)\n",
    "        #for each folder run the MMR summarizer and generate the final summary\n",
    "        curr_folder = main_folder_path + \"/\" + folder\n",
    "\n",
    "        # find all files in the sub folder selected\n",
    "        l=[\"a\",\"b\",\"c\"]\n",
    "        files=[]\n",
    "        num=str(j)\n",
    "        for i in l:\n",
    "            files.append(num+i+\".txt\")\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for file in files:\n",
    "            sentences = sentences + processFile(curr_folder + \"/\" + file)\n",
    "\n",
    "        # calculate TF, IDF and TF-IDF scores\n",
    "        # TF_w = TFs(sentences)\n",
    "        IDF_w = IDFs(sentences)\n",
    "        TF_IDF_w= TF_IDF(sentences)\n",
    "        #print(IDF_w)\n",
    "        #print(TF_IDF)\n",
    "        # build query; set the number of words to include in our query\n",
    "        query = buildQuery(sentences, TF_IDF_w, 100)\n",
    "        #print(\"Query=  \",query)\n",
    "        # pick a sentence that best matches the query\n",
    "        best1sentence = bestSentence(sentences, query, IDF_w)\n",
    "        #print(\"Best sentence  \",best1sentence)\n",
    "        # build summary by adding more relevant sentences\n",
    "        summary = makeSummary(sentences, best1sentence, query, 10, 0.5, IDF_w)\n",
    "        #print(\"Summary= \",summary)\n",
    "        final_summary = \"\"\n",
    "        for sent in summary:\n",
    "            final_summary = final_summary + sent.getOriginalWords() + \"\\n\"\n",
    "        final_summary = final_summary[:-1]\n",
    "        j=j+1\n",
    "        print(final_summary)\n",
    "        \n",
    "        #with open(os.path.join(results_folder,(str(folder) + \".MMR\")),\"w\") as fileOut: fileOut.write(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shreyabanerjee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# Program to measure the similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "# X = input(\"Enter first string: \").lower() \n",
    "# Y = input(\"Enter second string: \").lower() \n",
    "X =\"I love horror movies\"\n",
    "Y =\"I love horror movies\"\n",
    "\n",
    "# tokenization \n",
    "X_list = word_tokenize(X)  \n",
    "Y_list = word_tokenize(Y) \n",
    "  \n",
    "# sw contains the list of stopwords \n",
    "sw = stopwords.words('english')  \n",
    "l1 =[];l2 =[] \n",
    "  \n",
    "# remove stop words from the string \n",
    "X_set = {w for w in X_list if not w in sw}  \n",
    "Y_set = {w for w in Y_list if not w in sw} \n",
    "  \n",
    "# form a set containing keywords of both strings  \n",
    "rvector = X_set.union(Y_set)  \n",
    "for w in rvector: \n",
    "    if w in X_set: l1.append(1) # create a vector \n",
    "    else: l1.append(0) \n",
    "    if w in Y_set: l2.append(1) \n",
    "    else: l2.append(0) \n",
    "c = 0\n",
    "  \n",
    "# cosine formula  \n",
    "for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "print(\"similarity: \", cosine) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(X,Y):\n",
    "    # tokenization \n",
    "    X_list = word_tokenize(X)  \n",
    "    Y_list = word_tokenize(Y) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # remove stop words from the string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    print(\"similarity: \", cosine) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity:  0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity(\"Horror movies i love\", \"I love horror movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
